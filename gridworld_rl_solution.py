# -*- coding: utf-8 -*-
"""gridworld_RL_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13hDEBHYy3iKPxw5C0TNPMqXKI85ANgKX
"""

import numpy as np

# Define the grid dimensions and the rewards
grid_shape=(4, 4)
r_g=5
r_r=-5
r_s=-1
gamma=1  # Discount factor

# Initialize the values of all states to 0
V=np.zeros(grid_shape)
# Define the terminal states and their locations
terminal_states={(2, 3): r_g,(1, 0):r_r}  # states are in matrix index form
# Define walls (in matrix index form)
walls={(3, 0), (3, 1), (3, 2), (3, 3)}

# Define the actions and state transitions
def get_next_state(state, action):
    if state in terminal_states:
        return state  # Terminal state, no movement
    if action == 'up':
        next_state = (max(0, state[0] - 1), state[1])
    elif action == 'down':
        next_state = (min(grid_shape[0] - 1, state[0] + 1), state[1])
    elif action == 'left':
        next_state = (state[0], max(0, state[1] - 1))
    elif action == 'right':
        next_state = (state[0], min(grid_shape[1] - 1, state[1] + 1))

    # If next state is a wall, stay in the same state
    return next_state if next_state not in walls else state

# Value iteration function
def value_iteration(V, terminal_states, walls, theta=0.01, discount_factor=1):
    while True:
        delta = 0
        for i in range(grid_shape[0]):
            for j in range(grid_shape[1]):
                if (i, j) in terminal_states:
                    continue
                v = V[i, j]
                V[i, j] = max([r_s + discount_factor * V[get_next_state((i, j), action)]
                               for action in ['up', 'down', 'left', 'right']])
                delta = max(delta, abs(v - V[i, j]))
        if delta < theta:
            break
    return V

# Run value iteration
V_optimal = value_iteration(V, terminal_states, walls)

# Include the terminal states in the optimal value function
for state, reward in terminal_states.items():
    V_optimal[state] = reward

# Transform the value function to the state numbers used in the problem description
state_values = {i + j*grid_shape[1] + 1: V_optimal[i, j] for i in range(grid_shape[0]) for j in range(grid_shape[1])}
state_values

# Given the original values from part (a), we'll create a dictionary to represent them
original_values = {
    1: -1.0, 5: -2.0, 9: -3.0, 13: -2.0,
    2: -5.0, 6: -1.0, 10: -2.0, 14: -1.0,
    3: -1.0, 7: -2.0, 11: -1.0, 15: 5.0,
    4: -2.0, 8: -3.0, 12: -2.0, 16: -1.0
}

# Increment each value by 2 for each step taken under the original policy pi_g
new_values = {state: value + (2 * (abs(value) // 1)) for state, value in original_values.items()}

# Adjust the values for the terminal states directly based on the new rewards
new_values[12] = r_g + 2  # Green square
new_values[5] = r_r + 2   # Red square of death

new_values

